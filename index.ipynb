{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning and Pruning in Decision Trees - Lab \n",
    "\n",
    "## Introduction \n",
    "\n",
    "In this lab we will use the titanic dataset to see the impact of pruning and hyper parameter tuning on the predictive performance of decision tree classifier. \n",
    "\n",
    "Pruning reduces the size of the decision trees by removing nodes of the tree that do not provide power to classify instances. \n",
    "\n",
    "Decision trees are the most susceptible out of all the machine learning algorithms to overfitting, and effective pruning can reduce this likelihood. \n",
    "\n",
    "In this lab, we shall work with the Titanic dataset and see how we can tweak different hyper parameters for optimal pruning of the trees. \n",
    "\n",
    "## Objectives \n",
    "\n",
    "You will be able to: \n",
    "- Demonstrate how pruning is performed for decision trees\n",
    "- Understand and explain the role of different Decision tree hyperparameters \n",
    "- Select the best values for chosen hyperparamters and monitor the improvement in performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries \n",
    "\n",
    "Let's first import the libraries you would need for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/learn-env/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "//anaconda3/envs/learn-env/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib as mpl \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Titanic Dataset \n",
    "\n",
    "In the repo, we have made titanic dataset, all cleaned up and pre-processed for you, so that you can focus on pruning and optimization. The features set is available as `features.csv` and target variable as `target.csv`. \n",
    "\n",
    "* Load these files into separate dataframes below.\n",
    "* Check the shape for consistency and view the head. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 13) (891, 1)\n",
      "   PassengerId   Age  SibSp  Parch     Fare  Pclass_1  Pclass_2  Pclass_3  \\\n",
      "0            1  22.0      1      0   7.2500         0         0         1   \n",
      "1            2  38.0      1      0  71.2833         1         0         0   \n",
      "2            3  26.0      0      0   7.9250         0         0         1   \n",
      "3            4  35.0      1      0  53.1000         1         0         0   \n",
      "4            5  35.0      0      0   8.0500         0         0         1   \n",
      "\n",
      "   Sex_female  Sex_male  Embarked_C  Embarked_Q  Embarked_S  \n",
      "0           0         1           0           0           1  \n",
      "1           1         0           1           0           0  \n",
      "2           1         0           0           0           1  \n",
      "3           1         0           0           0           1  \n",
      "4           0         1           0           0           1  \n",
      "   Survived\n",
      "0         0\n",
      "1         1\n",
      "2         1\n",
      "3         1\n",
      "4         0\n"
     ]
    }
   ],
   "source": [
    "# Load features and target variables \n",
    "feat = pd.read_csv('features.csv')\n",
    "target = pd.read_csv('target.csv')\n",
    "print(feat.shape, target.shape)\n",
    "print(feat.head())\n",
    "print(target.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great. Now that we have our x (feat) and y (target), we can go ahead and make a split necessary for supervised learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a 70/30 train/test split \n",
    "\n",
    "* Using features and target variables above, create a 70/30 split using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 70/30 split for given X and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(feat, target, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we have our data ready for training, let's first train a DT classifier with this data. \n",
    "\n",
    "## Train a Vanilla Classifier \n",
    "\n",
    "__Note:__ The term \"vanilla\" is used for a machine learning algorithm with its default settings (no tweaking/tuning).\n",
    "\n",
    "- Create a decision tree instance. \n",
    "- Fit a DT classifier with training dataset using all default settings for hyperparameters i.e. we don't change any parameter. \n",
    "- Set the impurity criteria to \"entropy\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier using training data \n",
    "\n",
    "# instantiate \n",
    "dt = DecisionTreeClassifier(criterion='entropy')\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions \n",
    "- Create a set of predictions using the test set \n",
    "- Using `y_test` and `y_pred`, calculate the AUC (Area under curve) to check the predictive performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using test set \n",
    "y_pred = dt.predict(x_test)\n",
    "# Check the AUC of predictions\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Tree Depth\n",
    "\n",
    "Let's first check for the best depth parameter for our decision tree. \n",
    "- Create an array for for depth values ranging from 1 - 32. \n",
    "- In a loop, train the classifier for each depth value (32 runs) \n",
    "- Calculate the training and test AUC for each run \n",
    "- Plot a graph to show under/over fitting and optimal value \n",
    "- Interpret the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the optimal tree depth for given data\n",
    "max_depths = np.linspace(1, 32, 32, endpoint=True)\n",
    "train_results = []\n",
    "test_results = []\n",
    "for max_depth in max_depths:\n",
    "   dt = DecisionTreeClassifier(criterion='entropy', max_depth=max_depth)\n",
    "   dt.fit(x_train, y_train)\n",
    "   train_pred = dt.predict(x_train)\n",
    "   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
    "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "   # Add auc score to previous train results\n",
    "   train_results.append(roc_auc)\n",
    "   y_pred = dt.predict(x_test)\n",
    "   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "   # Add auc score to previous test results\n",
    "   test_results.append(roc_auc)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(max_depths, train_results, 'b', label='Train AUC')\n",
    "plt.plot(max_depths, test_results, 'r', label='Test AUC')\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You observations here \n",
    "\n",
    "# Training error decreases with increasing tree depth - clear sign of over fitting \n",
    "# Test error increases after depth=3 - nothing more to learn from deeper trees (some fluctuations, but not stable)\n",
    "# Optimal value seen here is 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum Sample Split\n",
    "\n",
    "Now check for the best `min_samples_splits` parameter for our decision tree. \n",
    "- Create an array for for `min_sample_splits` values ranging from 0.1 - 1 with an increment of 0.1 \n",
    "- In a loop, train the classifier for each `min_samples_splits` value (10 runs) \n",
    "- Calculate the training and test AUC for each run \n",
    "- Plot a graph to show under/over fitting and optimal value \n",
    "- Interpret the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the optimal min-samples-split for given data\n",
    "min_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)\n",
    "train_results = []\n",
    "test_results = []\n",
    "for min_samples_split in min_samples_splits:\n",
    "   dt = DecisionTreeClassifier(criterion='entropy', min_samples_split=min_samples_split)\n",
    "   dt.fit(x_train, y_train)\n",
    "   train_pred = dt.predict(x_train)\n",
    "   false_positive_rate, true_positive_rate, thresholds =    roc_curve(y_train, train_pred)\n",
    "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "   train_results.append(roc_auc)\n",
    "   y_pred = dt.predict(x_test)\n",
    "   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "   test_results.append(roc_auc)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(min_samples_splits, train_results, 'b', label='Train AUC')\n",
    "plt.plot(min_samples_splits, test_results, 'r', label='Test AUC')\n",
    "plt.xlabel('Min. Sample splits')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your observations\n",
    "\n",
    "# AUC for both test and train data stabilizes at 0.5 \n",
    "# Further increase in minimum sample split does not improve learning \n",
    "# Optimal value is .5 (or .6 - always a good idea not to choose the boundary value) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum Sample Leafs\n",
    "\n",
    "Now check for the best `min_samples_leafs` parameter value for our decision tree. \n",
    "- Create an array for for `min_samples_leafs` values ranging from 0.1 - 0.5 with an increment of 0.1 \n",
    "- In a loop, train the classifier for each `min_samples_leafs` value (5 runs) \n",
    "- Calculate the training and test AUC for each run \n",
    "- Plot a graph to show under/over fitting and optimal value \n",
    "- Interpret the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the optimal value for minimum sample leafs\n",
    "\n",
    "min_samples_leafs = np.linspace(0.1, 0.5, 5, endpoint=True)\n",
    "train_results = []\n",
    "test_results = []\n",
    "for min_samples_leaf in min_samples_leafs:\n",
    "   dt = DecisionTreeClassifier(criterion='entropy', min_samples_leaf=min_samples_leaf)\n",
    "   dt.fit(x_train, y_train)\n",
    "   train_pred = dt.predict(x_train)\n",
    "   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
    "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "   train_results.append(roc_auc)\n",
    "   y_pred = dt.predict(x_test)\n",
    "   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "   test_results.append(roc_auc)\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(12,6))    \n",
    "plt.plot(min_samples_leafs, train_results, 'b', label='Train AUC')\n",
    "plt.plot(min_samples_leafs, test_results, 'r', label='Test AUC')\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('Min. Sample Leafs')\n",
    "plt.legend()\n",
    "plt.show()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your observations here \n",
    "\n",
    "# AUC gives best value between 0.2 and 0.3 for both test and training sets \n",
    "# The accuracy drops down if we continue to increase the parameter value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Features\n",
    "\n",
    "Now check for the best `max_features` parameter value for our decision tree. \n",
    "- Create an array for for `max_features` values ranging from 1 - 12 (1 features vs all)\n",
    "- In a loop, train the classifier for each `max_features` value (12 runs) \n",
    "- Calculate the training and test AUC for each run \n",
    "- Plot a graph to show under/over fitting and optimal value \n",
    "- Interpret the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best value for optimal maximum feature size\n",
    "max_features = list(range(1,x_train.shape[1]))\n",
    "train_results = []\n",
    "test_results = []\n",
    "for max_feature in max_features:\n",
    "   dt = DecisionTreeClassifier(criterion='entropy', max_features=max_feature)\n",
    "   dt.fit(x_train, y_train)\n",
    "   train_pred = dt.predict(x_train)\n",
    "   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
    "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "   train_results.append(roc_auc)\n",
    "   y_pred = dt.predict(x_test)\n",
    "   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "   roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "   test_results.append(roc_auc)\n",
    "\n",
    "    \n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(max_features, train_results, 'b', label='Train AUC')\n",
    "plt.plot(max_features, test_results, 'r', label='Test AUC')\n",
    "\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('max features')\n",
    "plt.legend()\n",
    "plt.show()    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your observations here \n",
    "\n",
    "# No clear effect on the training dataset - flat AUC \n",
    "# SOme fluctuations in test AUC but not definitive enough to make a judgement\n",
    "# highest AUC value seen at 5/7. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-train the classifier with chosen values\n",
    "\n",
    "So now we shall use the best values from each training phase above and feed it back to our classifier and see if have any improvement in predictive performance. \n",
    "\n",
    "- Train the classifier with optimal values identified \n",
    "- compare the AUC with vanilla DT AUC \n",
    "- Interpret the results of comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a classifier with optimal values identified above\n",
    "dt = DecisionTreeClassifier(criterion='entropy',\n",
    "                           max_features=7,\n",
    "                           max_depth=3,\n",
    "                           min_samples_split=0.6,\n",
    "                           min_samples_leaf=0.25)\n",
    "dt.fit(x_train, y_train)\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You observations here \n",
    "# we moved from AUC 0.69 in the vanilla classifier to 0.73 some tuning. \n",
    "# Due to randomness, results may slightly differ, there is some improvement in most cases. \n",
    "# With more complicated datasets, we might see an even bigger improvement in AUC/accuracy of the classifier. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we shall talk about hyper-parameter tuning using a technique called \"grid-search\" to make this process even more granular and decisive. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "In this lesson, we looked at tuning a decision tree classifier in order to avoid over fitting and increasing the generalization capabilities of the classifier. For the Titanic dataset, we see that identifying optimal parameter values can result in some improvements towards predictions. This idea will be exploited further in upcoming lessons and labs. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
